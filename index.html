<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>ASL Sign Language Detection</title>

    <!-- Bootstrap Core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">

    <!-- Theme CSS -->
    <link href="css/grayscale.min.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->

    </head>

    <body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

        <!-- Navigation -->
        <nav class="navbar navbar-custom navbar-fixed-top" role="navigation">
            <div class="container">

                <!-- Collect the nav links, forms, and other content for toggling -->
                <div class="collapse navbar-collapse navbar-right navbar-main-collapse">
                    <ul class="nav navbar-nav">
                        <!-- Hidden li included to remove active class from about link when scrolled up past about section -->
                        <li class="hidden">
                            <a href="#page-top"></a>
                        </li>

                        <li> <a class="page-scroll"  href = "#discussion">Discussion</a></li>
                        <li> <a class="page-scroll"  href = "#datasets">Datasets </a></li>
                        <li> <a class="page-scroll"  href = "#videos">Videos</a></li>
                        <li> <a class="page-scroll" href = "#references">References</a></li>
                        <li><a class="page-scroll"  href="#github-repo">Code</a></li>
                        <li><a class="page-scroll"  href="#midterm-report">Mid-term report</a></li>
                        <li><a class="page-scroll" href="#presentation">Final presentation</a></li>
                    </ul>
                </div>
            </div>
        </nav>

        <!-- Intro Header -->
        <header class="intro">
            <div class="intro-body">
                <div class="container">
                    <div class="row">
                        <div class="col-md-8 col-md-offset-2">
                            <h1 class="brand-heading">ASL Sign Language Detection</h1>
                            <p class="intro-text">
                                <ul>
                                    <p>Ancy Philip  <a class = "glyphicon glyphicon-envelope" href="mailto:aphilip@wisc.edu"></a></p>
                                    <p>Aribhit Mishra  <a class = "glyphicon glyphicon-envelope"  href="mailto:amishra28@wisc.edu"></a></p>
                                    <p>Pavan Kemparaju  <a  class = "glyphicon glyphicon-envelope" href="mailto:kemparaju@wisc.edu"></a></p>
                                </ul>
                            </p>
                            <a href="#about" class="btn btn-circle page-scroll">
                                <i class="fa fa-angle-double-down animated"></i>
                            </a>
                        </div>
                    </div>
                </div>
            </div>
        </header>

        <!-- About Section -->
        <section id="discussion" class="container content-section text-center">
            <div class="row">
                <div class="col-lg-12 ">
                    <h2>Discussion</h2>
                    <div>

                      <!-- Nav tabs -->
                      <ul class="nav nav-tabs" role="tablist">
                        <li role="presentation" class="active"><a href="#Motivation" aria-controls="home" role="tab" data-toggle="tab">Motivation</a></li>
                        <li role="presentation"><a href="#LiteratureSurvey" aria-controls="profile" role="tab" data-toggle="tab">Literature Survey</a></li>
                        <li role="presentation"><a href="#Approach" aria-controls="messages" role="tab" data-toggle="tab">Approach</a></li>
                        <li role="presentation"><a href="#Challenges" aria-controls="settings" role="tab" data-toggle="tab">Challenges</a></li>
                    </ul>

                    <!-- Tab panes -->
                    <div class="tab-content">
                        <div role="tabpanel" class="tab-pane active text-justify " id="Motivation">
                            <p>
                                Our project aims to detect hand sign language (based on the American Sign Language system, ASL) from a captured image, and possibly extend it to videos, using image processing and machine learning techniques.
                            </p>
                            <p>
                                There are many different sign language standards being used all over the world. We have decided to pick ASL as it is one of the major and more popular forms of communication for deaf people in the USA and Canada.
                            </p>
                            <p>
                                The current sign language detection systems are trained to have a good precision on one setof symbols (For eg. a-g of the ASL alphabet)
                                Sign language detection techniques aren’t restricted to language translation and can be extended to support other actions such as actions for a computer.
                            </p>
                            <p>
                                Besides this, we like the social impact of this project in improving communication and accessibility.
                            </p>
                        </div>
                        <div role="tabpanel" class="tab-pane text-justify " id="LiteratureSurvey">
                            <p>
                                Initially, we studied research work that could provide us a basis for methods to extract the ‘Hand’ object from the background. Kakumanu et al [1] perform a review of various skin modeling and classification strategies based on color information in the visual spectrum. From the compared skin detection methods, Skin Probability Maps seems to be the quickest and has the best accuracy. We decided to experiment with this algorithm to isolate the hand. 
                            </p>
                            <p>
                                Proceeding with the theme of skin detection; Starting with normalized RGB components, Gomez and Moralez [2] came up with a single rule to detect skin in images. The work finds an initial threshold to be used for Skin Probability Map which is more effective than other measures. We used this threshold value and tuned it (based on further experiments) to isolate the hand.
                            </p>
                            <p>
                                R. Collins et al [3] propose another way to extract the hand; through background subtraction. The algorithm is useful for detecting moving objects in a scene under controlled conditions. We used the Background Subtraction method mentioned to detect the hand by constraining our data such that the only moving object in it is the hand making the signs.
                            </p>
                            <p>
                                Finally, E. Stergiopoulou et al [4] present a combination of existing techniques, based on motion detection and a novel skin color classifier to improve segmentation accuracy which potentially works even on noisy backgrounds. We used the method combining image differencing with background subtraction with adaptive thresholds described in this paper.
                            </p>
                            <p>
                                After extract the hand, we need to select features for classification. We feel that the features used need to generalise well, and so we resorted to using standard features such as Gabor Filters, Histogram of Centroid Distances and SURF. For the purpose of classification, we have used standard machine learning algorithms like K-nearest neighbour (KNN), Support Vector Machine(SVM), Convolutional Neural Nets and Random Forest.
                            </p>
                            <p>
                                All the filters and learning algorithms are applied separately and the accuracies on the datasets have been measured to determine the best methods going forward.
                            </p>
                        </div>
                        <div role="tabpanel" class="tab-pane text-justify " id="Approach">
                           <p> After going through the literature and performing some initial tests, it was decided to divide the project into the following three stages.<p>
                               <ol class="banner-social-buttons">
                                <li>Isolating hand from background</li>
                                <li>Extracting features from hand image for classification</li>
                                <li> Classification using Machine Learning Algorithms</li>
                            </ol>
                            <p>
                                Each subsequent section of this discussion describes the stages in more detail.
                                Our target to is to make the system work with no specialized hardware. Thus, we use Integrated webcam to capture image and video input for testing.
                            </p>
                            <p>
                                Current testing has only be done on images taken under controlled conditions (same lighting, hand in the center of the image, black background), but we intend to extend it to images over different lighting conditions</p>
                                <p>
                                    All coding is done in MATLAB 2017a with Image Acquisition Toolkit<br>
                                    We made the following observations about the algorithms used:
                                </p>


                                <table class="table ">
                                    <thead> 
                                        <tr> <th>#</th>
                                         <th>First Name</th> 
                                         <th>Last Name</th> 
                                         <th>Username</th> 
                                     </tr> 
                                 </thead> 
                                 <tbody> <tr> <th scope="row">1</th> <td>Mark</td> <td>Otto</td> <td>@mdo</td> </tr> <tr> <th scope="row">2</th> <td>Jacob</td> <td>Thornton</td> <td>@fat</td> </tr> <tr> <th scope="row">3</th> <td>Larry</td> <td>the Bird</td> <td>@twitter</td> </tr> </tbody> </table>

                             </p>
                         </div>
                         <div role="tabpanel" class="tab-pane text-justify " id="Challenges">

                             <p>
                                 <h5>Isolating hand from background </h5>
                             </p>   
                             <p> 
                                Any one method for hand isolation does not seem to work well enough that we can feed the result to a classifier. Hence we have had to combine two methods for isolation and consider other methods.
                                Skin Probability Mapping
                                The result is dependent on the initial threshold values for color components. Which means this method will not generalize well for all skin tones. Also objects in the scene whose color is close to the skin color threshold act as noise.
                                The algorithm is not robust to changes in lighting conditions since it works with hue and saturation.

                                <img src="img/SPM_bbg.png" class="img-responsive "  alt="Responsive image">


                                Image differencing
                                This method is useful only if the hand is moving, so other moving elements in the scene add noise.
                                Background subtraction
                                This method does not take into account changes in lighting in the scene due to movement.
                                The approach is highly sensitive to the initial threshold and adaptation speed and the values we have used are empirically chosen.
                                The background subtraction algorithm fails to extract the entire hand when the latter moves over background objects of a similar color

                            </p>
                            <p>
                                <h5>Extracting Features for Classification </h5>
                            </p>
                            <p>
                                HOCD
                                HOCD does not generalise well on new hands.
                                For some fingerspelling gestures like ’m’ and ’n’ which have almost similar shape, HOCD would give a similar feature vector and hence, we would have a bad classification using only these features.
                                Gabor Filters
                                The feature extraction system is not invariant to rotation, scaling and translational changes. Scaling and translation can be compensated easily enough as soon as the hand has been correctly segmented. However, rotation differences are difficult to detect on the segmented hand and compensate and so it remains a problem.
                                SURF and Bag of words
                                It was reasoned out that these features are not suitable for actual implementation, because they fail to capture enough information from the images because of illumination differences playing a major role in different key-points extracted on train and test images.
                            </p>
                            <p>
                                <h5>Classification using Machine Learning Algorithms </h5>
                            </p>
                            <p>
                                We observed that CNN gives the best result followed by KNN , followed by SVM, followed by Random Forest.<br>
                                CNN works very well if the background does not add any noise to the hand sign. It performs well (0.9+ accuracy) if the nature of the background is similar in training and test images. Any change in background from the training to the testing causes fall in accuracy (66%), i.e. CNNs do not generalize well.<br>
                                The other classifiers depend heavily on the features which in turn depend heavily on the nature of the images. The current set of features we have experimented with are all susceptible to noise. <br>
                            </p>
                        </div>
                    </div>

                </div>

            </div>


        </div>
    </div>
</section>



<!-- Datasets Section -->
<section id="datasets" class="content-section text-center">
    <div class="download-section">
        <div class="container">
            <div class="col-lg-12 col-lg-offset-1">
                <h2>Datasets</h2>
                <div class="col-md-12">
                    <a class="btn btn-info btn-lg" href="http://www.massey.ac.nz/~albarcza/gesture_dataset2012.html"><h4>Massey</h4></a>
                    <a class="btn btn-info btn-lg" href="http://empslocal.ex.ac.uk/people/staff/np331/index.php?section=ComplexbackgroundDataset"><h4>Complex Background</h4></a>
                    <a class="btn btn-info btn-lg" href="https://www.ece.nus.edu.sg/stfpage/elepv/NUS-HandSet/"><h4> NUS Hand</h4></a>
                    <a class="btn btn-info btn-lg" href="https://www.dropbox.com/sh/qzr8jw06ol2ztys/AABSjcY4JuEbu5-_TICXI9gJa?dl=0"><h4>Created Test Dataset</h4></a>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Videos Section -->
<section id="videos" class="content-section text-center">
    <div class="content-section">
        <div class="container vid">
            <h2>Videos</h2>
            <div class="row">
                <div class="col-md-6">
                    <video width="320" height="240" controls="">
                      <source src="videos/image difference.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <h5>Image differencing</h5>
                </div>

                <div class="col-md-6">
                    <video width="320" height="240" controls="">
                      <source src="videos/hand isolation.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <h5>Hand isolation</h5>
                </div>
            </div>
            <div class="row">
                <div class="col-md-6">
                    <video width="320" height="240" controls="">
                      <source src="videos/hand tracking.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <h5>Hand Tracking</h5>
                </div>

                <div class="col-md-6">
                    <video width="320" height="240" controls="" src="videos/vid1.mov">
                        Your browser does not support the video tag.
                    </video>
                    <h5>Final Result</h5>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- References Section -->
<section id="references" class="content-section text-center">
    <div>
        <div class="container">
            <div class="col-lg-12 col-lg-offset-1">
                <h2>References</h2>
                <div class="col-md-12">
                    <p><a href="https://s3.amazonaws.com/academia.edu.documents/44580807/A_survey_of_skin-color_modeling_and_dete20160409-14723-aklewp.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1494291484&Signature=4ExfC1S9fG4kbALXIKzV87fF9F4%3D&response-content-disposition=inline%3B%20filename%3DA_survey_of_skin-color_modeling_and_dete.pdf">[ 1 ] Kakumanu, Praveen, Sokratis Makrogiannis, and Nikolaos Bourbakis. "A survey of skin-color modeling and detection methods." Pattern recognition 40.3 (2007): 1106-1122.
                    </a></p>

                    <p><a href="http://www.cse.unsw.edu.au/~icml2002/workshops/MLCV02/MLCV02-Morales.pdf">[ 2 ]Gomez, Giovani, and Eduardo Morales. "Automatic feature construction and a simple rule induction algorithm for skin detection." Proc. of the ICML workshop on Machine Learning in Computer Vision. Vol. 31. 2002.
                    </a></p>
                    <p><a href="http://www.cs.stevens-tech.edu/~kamberov/Papers/ASystemforVideoSurveillanceAndMonitoring.pdf">[ 3 ]Collins, Robert T., et al. "A system for video surveillance and monitoring." (2000): 1456-1471.
                    </a></p>
                    <p><a href="https://www.researchgate.net/profile/Nikos_Papamarkos/publication/263088565_Real_Time_Hand_Detection_in_a_Complex_Background/links/00b7d53c306675af94000000.pdf">[ 4 ]Stergiopoulou, Ekaterini, et al. "Real time hand detection in a complex background." Engineering Applications of Artificial Intelligence 35 (2014): 54-70.</a></p>
                    <p><a href="http://www.massey.ac.nz/~albarcza/gesture_dataset2012.html">[ 5 ]Massey Dataset</a></p>
                    <p><a href="http://empslocal.ex.ac.uk/people/staff/np331/index.php?section=ComplexbackgroundDataset">[ 6 ]Hand with complex Background</a></p>
                    <p><a href="https://www.ece.nus.edu.sg/stfpage/elepv/NUS-HandSet/">[ 7 ]NUS dataset</a></p>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Code -->
<section id="github-repo" class="content-section text-center">
    <div>
        <div class="container">
            <div class="col-lg-12 col-lg-offset-1">
                <h2>Code</h2>
                <div class="col-md-12">
                    <a href="https://github.com/pavankm/CS766/tree/raw_code" class="btn btn-default btn-lg"><i class="fa fa-github fa-fw"></i> <span class="network-name">Github</span></a>                    
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Midterm Report -->
<section id="midterm-report" class="content-section text-center">
    <div>
        <div class="container">
            <div class="col-lg-12 col-lg-offset-1">
                <h2>Mid-term Report</h2>
                <div class="col-md-12">
                    <p><a href="https://docs.google.com/document/d/1JizHbI3bwTzVAXLTrrCnwG7VZqRMWIeOCLJnGV_sru4/edit?usp=sharing">Link</a><br></p>
                    <iframe src="https://docs.google.com/document/d/1JizHbI3bwTzVAXLTrrCnwG7VZqRMWIeOCLJnGV_sru4/pub?embedded=true" width="960" height="569" ></iframe>
                </div>
            </div>
        </div>
    </div>
</section>


<!-- Final Presentation -->
<section id="presentation" class="content-section text-center">
    <div>
        <div class="container">
            <div class="col-lg-12 col-lg-offset-1">
                <h2>Final Presentation</h2>
                <div class="col-md-12">
                   <p><a href="https://docs.google.com/presentation/d/1iZVdmmRZLHIcgmz1H7jhqcJICaX7vaCjfe4hiHo61mA/edit?usp=sharing">Link</a><br></p>
                   <iframe src="https://docs.google.com/presentation/d/1iZVdmmRZLHIcgmz1H7jhqcJICaX7vaCjfe4hiHo61mA/embed?start=false&loop=false&delayms=3000" frameborder="0" width="960" height="569" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>
               </div>
           </div>
       </div>
   </div>
</section>



<!-- Contact Section -->
<section id="contact" class="container content-section text-center">
    <div class="row">
        <div class="col-lg-8 col-lg-offset-2">
            <h2>Contact Start Bootstrap</h2>
            <p>Feel free to email us to provide some feedback on our templates, give us suggestions for new templates and themes, or to just say hello!</p>
            <p><a href="mailto:feedback@startbootstrap.com">feedback@startbootstrap.com</a>
            </p>
            <ul class="list-inline banner-social-buttons">
                <li>
                    <a href="https://twitter.com/SBootstrap" class="btn btn-default btn-lg"><i class="fa fa-twitter fa-fw"></i> <span class="network-name">Twitter</span></a>
                </li>
                <li>
                    <a href="https://github.com/IronSummitMedia/startbootstrap" class="btn btn-default btn-lg"><i class="fa fa-github fa-fw"></i> <span class="network-name">Github</span></a>
                </li>
                <li>
                    <a href="https://plus.google.com/+Startbootstrap/posts" class="btn btn-default btn-lg"><i class="fa fa-google-plus fa-fw"></i> <span class="network-name">Google+</span></a>
                </li>
            </ul>
        </div>
    </div>
</section>


<!-- Footer -->
<footer>
    <div class="container text-center">
        <p>ASL Sign Language Detection</p>
        <p>CS766 Spring 2017</p>
    </div>
</footer>

<!-- jQuery -->
<script src="vendor/jquery/jquery.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="vendor/bootstrap/js/bootstrap.min.js"></script>

<!-- Plugin JavaScript -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>

<!-- Google Maps API Key - Use your own API key to enable the map feature. More information on the Google Maps API can be found at https://developers.google.com/maps/ -->
<script type="text/javascript" src="https://maps.googleapis.com/maps/api/js?key=AIzaSyCRngKslUGJTlibkQ3FkfTxj3Xss1UlZDA&sensor=false"></script>

<!-- Theme JavaScript -->
<script src="js/grayscale.min.js"></script>

</body>

</html>
